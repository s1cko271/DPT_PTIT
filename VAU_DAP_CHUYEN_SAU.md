# üéµ H·ªÜ TH·ªêNG T√åM KI·∫æM NH·∫†C T∆Ø∆†NG T·ª∞ - T√ÄI LI·ªÜU V·∫§N ƒê√ÅP CHUY√äN S√ÇU

## üìã T·ªîNG QUAN H·ªÜ TH·ªêNG

### üéØ M·ª•c ti√™u Ch√≠nh
X√¢y d·ª±ng h·ªá th·ªëng **Content-Based Music Information Retrieval (CBMIR)** s·ª≠ d·ª•ng k·ªπ thu·∫≠t ph√¢n t√≠ch √¢m thanh theo t·ª´ng **bar** v·ªõi **overlap 50%** ƒë·ªÉ t√¨m ki·∫øm c√°c b√†i h√°t c√≥ ƒë·∫∑c ƒëi·ªÉm t∆∞∆°ng t·ª±.

### üèóÔ∏è Ki·∫øn Tr√∫c T·ªïng Quan
```
Input Audio ‚Üí Feature Extraction ‚Üí Database Storage ‚Üí Similarity Search ‚Üí Ranked Results
```

### üìä Th·ªëng K√™ H·ªá Th·ªëng
- **Dataset**: 130 b√†i h√°t, 10 th·ªÉ lo·∫°i (blues, classical, country, disco, hiphop, jazz, metal, pop, reggae, rock)
- **Features**: 151 ƒë·∫∑c tr∆∞ng m·ªói b√†i h√°t
- **Processing Time**: ~1-2 gi√¢y cho m·ªôt query
- **Accuracy**: T·ªët cho demo, similar genres cluster together

---

## üéµ KI·∫æN TH·ª®C CHUY√äN S√ÇU V·ªÄ ƒê·∫∂C TR∆ØNG √ÇM THANH

### 1. RMS Energy (Root Mean Square Energy)
**C√¥ng th·ª©c to√°n h·ªçc:**
```
RMS = ‚àö(1/N √ó Œ£(xi¬≤))
```
**√ù nghƒ©a v·∫≠t l√Ω:**
- ƒêo l∆∞·ªùng **nƒÉng l∆∞·ª£ng trung b√¨nh** c·ªßa t√≠n hi·ªáu √¢m thanh
- T∆∞∆°ng quan v·ªõi **loudness** m√† tai ng∆∞·ªùi nghe ƒë∆∞·ª£c
- **Range**: 0.0 - 1.0 (sau normalization)

**·ª®ng d·ª•ng th·ª±c t·∫ø:**
- Rock/Metal: RMS cao (0.15-0.25)
- Classical/Ambient: RMS th·∫•p (0.05-0.15)
- Silence: RMS ‚âà 0

### 2. Zero-Crossing Rate (ZCR)
**C√¥ng th·ª©c:**
```
ZCR = 1/2N √ó Œ£|sign(x[n]) - sign(x[n-1])|
```
**√ù nghƒ©a:**
- ƒê·∫øm s·ªë l·∫ßn t√≠n hi·ªáu **ƒë·ªïi d·∫•u** trong m·ªôt ƒë∆°n v·ªã th·ªùi gian
- Ph·∫£n √°nh **texture** c·ªßa √¢m thanh (smooth vs rough)

**Ph√¢n t√≠ch theo lo·∫°i √¢m:**
- **Voiced sounds** (vocal): ZCR th·∫•p (~0.02-0.05)
- **Unvoiced sounds** (s, f, h): ZCR cao (~0.15-0.30)  
- **Instrumental**: ZCR trung b√¨nh (~0.05-0.15)

### 3. Spectral Centroid
**ƒê·ªãnh nghƒ©a:**
"Tr·ªçng t√¢m" c·ªßa ph·ªï t·∫ßn s·ªë - t·∫ßn s·ªë m√† t·∫°i ƒë√≥ t·∫≠p trung nhi·ªÅu nƒÉng l∆∞·ª£ng nh·∫•t.

**C√¥ng th·ª©c:**
```
SC = Œ£(f[k] √ó X[k]) / Œ£(X[k])
```
Trong ƒë√≥:
- f[k]: T·∫ßn s·ªë th·ª© k
- X[k]: Magnitude c·ªßa FFT t·∫°i bin k

**√ù nghƒ©a √¢m nh·∫°c:**
- **Brightness**: Cao = s√°ng, th·∫•p = t·ªëi
- **Instrumental timbre**: Piano (cao), Bass guitar (th·∫•p)

### 4. Spectral Bandwidth
**C√¥ng th·ª©c:**
```
SB = ‚àö(Œ£((f[k] - SC)¬≤ √ó X[k]) / Œ£(X[k]))
```
**√ù nghƒ©a:**
- ƒêo **ƒë·ªô r·ªông ph·ªï t·∫ßn s·ªë** quanh spectral centroid
- Ph·∫£n √°nh **complexity** c·ªßa harmonics

**Ph√¢n t√≠ch:**
- **Pure tones**: Bandwidth th·∫•p
- **Noise/Complex sounds**: Bandwidth cao
- **Multi-instrumental**: Bandwidth trung b√¨nh ƒë·∫øn cao

### 5. Spectral Rolloff
**ƒê·ªãnh nghƒ©a:**
T·∫ßn s·ªë m√† t·∫°i ƒë√≥ 85% t·ªïng nƒÉng l∆∞·ª£ng ph·ªï ƒë∆∞·ª£c t·∫≠p trung.

**Algorithm:**
```python
cumulative_energy = cumsum(spectrum)
rolloff_85 = frequencies[cumulative_energy >= 0.85 * total_energy][0]
```

**·ª®ng d·ª•ng:**
- **High-frequency instruments**: Rolloff cao (cymbals, hi-hats)
- **Low-frequency instruments**: Rolloff th·∫•p (bass, cello)

---

## üîÑ PH∆Ø∆†NG PH√ÅP BAR-BASED EXTRACTION

### üéº L√Ω Thuy·∫øt √Çm Nh·∫°c
**Bar (Measure)**: ƒê∆°n v·ªã th·ªùi gian c∆° b·∫£n trong √¢m nh·∫°c
- **4/4 time signature**: 4 beats per bar
- **Tempo 120 BPM**: 1 beat = 0.5 gi√¢y ‚Üí 1 bar = 2 gi√¢y

### üîß Technical Implementation

#### Tham S·ªë C·∫•u H√¨nh
```python
BPM = 120                    # C·ªë ƒë·ªãnh cho consistency
beats_per_bar = 4           # 4/4 time signature
bar_duration = 60/BPM * 4   # = 2.0 gi√¢y
overlap_ratio = 0.5         # 50% overlap
num_bars = 29               # T·ªëi ∆∞u cho 60-second songs
```

#### Thu·∫≠t To√°n Chia Bar
```python
def create_bars(audio_signal, sample_rate):
    bar_samples = int(2.0 * sample_rate)      # 2 gi√¢y
    hop_samples = bar_samples // 2            # 1 gi√¢y hop
    
    bars = []
    for i in range(29):
        start = i * hop_samples
        end = start + bar_samples
        bar = audio_signal[start:end]
        
        # Padding n·∫øu thi·∫øu data
        if len(bar) < bar_samples:
            bar = np.pad(bar, (0, bar_samples - len(bar)))
        
        bars.append(bar)
    
    return bars
```

### üìä ∆Øu ƒêi·ªÉm Bar-Based Approach

1. **Temporal Consistency**: M·ªói bar c√≥ ƒë·ªô d√†i c·ªë ƒë·ªãnh (2 gi√¢y)
2. **Musical Alignment**: Theo structure t·ª± nhi√™n c·ªßa nh·∫°c
3. **Overlap Benefits**: Kh√¥ng b·ªè s√≥t th√¥ng tin ·ªü ranh gi·ªõi
4. **Scalability**: D·ªÖ parallel processing t·ª´ng bar
5. **Detail Capture**: 29 bars √ó 5 features = 145 temporal features

### üéØ Feature Vector Final Structure
```
[Bar1_Features] [Bar2_Features] ... [Bar29_Features] [Summary_Features] [BPM]
     5 vals         5 vals            5 vals            5 vals        1 val
     
Total: 29√ó5 + 5 + 1 = 151 features
```

---

## üíæ C∆† S·ªû D·ªÆ LI·ªÜU ƒêA PH∆Ø∆†NG TI·ªÜN

### üóÑÔ∏è Database Schema
```sql
CREATE TABLE songs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    filename TEXT NOT NULL,               -- blues.00000.wav
    title TEXT,                          -- blues 00000  
    genre TEXT,                          -- blues
    features BLOB NOT NULL,              -- Serialized feature dict
    feature_count INTEGER,               -- 151
    feature_version TEXT,                -- "bars_v1"
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### üîê BLOB Storage Strategy

#### Serialization Process
```python
import joblib
import io

# Serialize complex Python object
feature_dict = {
    'bpm': 120,
    'num_bars': 29,
    'bar_features': [...],           # 29 bars √ó 5 features
    'summary_vector': [...],         # 5 features
    'feature_vector': [...]          # 151 features
}

buffer = io.BytesIO()
joblib.dump(feature_dict, buffer)
features_blob = buffer.getvalue()   # Binary data for BLOB
```

#### Deserialization Process
```python
# Load from BLOB
features = joblib.load(io.BytesIO(features_blob))
feature_vector = features['feature_vector']  # Extract 151-d vector
```

### üìà Advantages of BLOB Approach

1. **Flexibility**: Store complex nested structures
2. **Efficiency**: Binary format, compressed
3. **Versioning**: Can store different feature versions
4. **Atomicity**: Single transaction per song
5. **Extensibility**: Easy to add new feature types

### üîç Indexing Strategy (for scaling)
```sql
-- Metadata indexes for fast filtering
CREATE INDEX idx_genre ON songs(genre);
CREATE INDEX idx_filename ON songs(filename);
CREATE INDEX idx_feature_version ON songs(feature_version);

-- For similarity search, would need:
-- - LSH (Locality Sensitive Hashing) 
-- - Vector databases (Faiss, Pinecone)
-- - Approximate nearest neighbors
```

---

## üîç THU·∫¨T TO√ÅN T√åM KI·∫æM

### üìê Cosine Similarity (Primary Method)

#### Mathematical Foundation
```
cos(Œ∏) = (A ¬∑ B) / (||A|| √ó ||B||)

Where:
- A, B: Feature vectors (151-dimensional)
- A ¬∑ B: Dot product
- ||A||: L2 norm of vector A
```

#### Normalization Process
```python
def normalize_vector(v):
    norm = np.sqrt(np.sum(v**2))
    if norm == 0:
        return v
    return v / norm

def cosine_similarity(a, b):
    a_norm = normalize_vector(a)
    b_norm = normalize_vector(b)
    return np.dot(a_norm, b_norm)
```

#### Why Cosine for Audio?
1. **Scale Invariant**: Kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi volume differences
2. **Direction Focus**: Quan t√¢m pattern, kh√¥ng ph·∫£i magnitude
3. **High Dimensional**: Hi·ªáu qu·∫£ v·ªõi 151-D vectors
4. **Robust**: Less sensitive to outlier features

### üìè Euclidean Distance (Alternative)

#### Formula & Implementation
```python
def euclidean_distance(a, b):
    return np.sqrt(np.sum((a - b)**2))

# Converted to similarity (0-1 range)
def euclidean_similarity(a, b):
    dist = euclidean_distance(a, b)
    return 1 / (1 + dist)  # Closer distance ‚Üí higher similarity
```

#### When to Use Euclidean?
- When **absolute differences** matter
- For **magnitude-sensitive** features
- With **pre-normalized** data

### üéØ Search Algorithm

#### Complete Pipeline
```python
def search_similar_songs(input_features, database_features, top_k=3):
    similarities = []
    
    # Normalize input
    input_norm = normalize_features(input_features)
    
    # Compare with all songs in database
    for song_id, song_features in database_features:
        song_norm = normalize_features(song_features)
        
        # Calculate similarity
        similarity = cosine_similarity(input_norm, song_norm)
        similarities.append((song_id, similarity))
    
    # Sort by similarity (descending)
    similarities.sort(key=lambda x: x[1], reverse=True)
    
    # Return top-k results
    return similarities[:top_k]
```

#### Time Complexity
- **Feature Extraction**: O(N) where N = audio length
- **Database Search**: O(M√óD) where M = #songs, D = feature dims
- **Sorting**: O(M log M)
- **Total**: O(N + M√óD + M log M)

For our case: N‚âà88,200 (2-sec audio), M=130, D=151
‚Üí Very fast, suitable for real-time

---

## üéõÔ∏è FEATURE NORMALIZATION

### üìä Min-Max Normalization
```python
def min_max_normalize(features):
    """Scale features to [0, 1] range"""
    min_val = np.min(features)
    max_val = np.max(features)
    
    if max_val == min_val:
        return np.zeros_like(features)
    
    return (features - min_val) / (max_val - min_val)
```

**Advantages:**
- Preserves relationships
- All features same scale
- Interpretable [0,1] range

### üìà Z-Score Normalization (Alternative)
```python
def z_score_normalize(features):
    """Standardize to mean=0, std=1"""
    mean = np.mean(features)
    std = np.std(features)
    
    if std == 0:
        return np.zeros_like(features)
    
    return (features - mean) / std
```

**When to use:**
- Features follow normal distribution
- Want to remove scale effects
- Preserve relative distances

### üéØ Why Normalization is Critical

**Problem without normalization:**
```
RMS Energy: 0.1 - 0.3 range
Spectral Centroid: 1000 - 5000 Hz range
‚Üí Centroid dominates similarity calculation
```

**Solution with normalization:**
```
All features: 0.0 - 1.0 range
‚Üí Equal weight in similarity calculation
```

---

## üöÄ PERFORMANCE OPTIMIZATION

### ‚ö° Real-time Processing Techniques

1. **Vectorized Operations**
```python
# Slow: Loop-based
for i in range(len(audio)):
    rms += audio[i] ** 2

# Fast: Vectorized
rms = np.sqrt(np.mean(audio ** 2))
```

2. **Pre-computed FFT Windows**
```python
# Pre-compute Hamming window
window = np.hamming(bar_length)  # Compute once

# Use in each bar
for bar in bars:
    windowed = bar * window      # Fast element-wise multiply
    spectrum = np.fft.rfft(windowed)
```

3. **Database Connection Pooling**
```python
# Connection reuse instead of open/close per query
class DatabaseManager:
    def __init__(self, db_path):
        self.conn = sqlite3.connect(db_path)
    
    def search(self, features):
        # Reuse existing connection
        cursor = self.conn.cursor()
        # ... search logic
```

### üìä Profiling Results
```
Feature Extraction: ~0.8s (80% of total time)
‚îú‚îÄ‚îÄ Audio loading: ~0.1s
‚îú‚îÄ‚îÄ Bar creation: ~0.1s  
‚îú‚îÄ‚îÄ FFT computation: ~0.4s
‚îî‚îÄ‚îÄ Feature calculation: ~0.2s

Database Search: ~0.2s (20% of total time)
‚îú‚îÄ‚îÄ Loading features: ~0.1s
‚îî‚îÄ‚îÄ Similarity calculation: ~0.1s
```

---

## üéØ VALIDATION & TESTING

### üß™ Accuracy Evaluation

#### Test Cases
```python
test_cases = {
    'blues.00000.wav': ['blues.00001.wav', 'blues.00013.wav'],
    'classical.00000.wav': ['classical.00001.wav', 'classical.00016.wav'],
    'rock.00000.wav': ['rock.00001.wav', 'rock.00014.wav']
}

def evaluate_accuracy():
    correct_predictions = 0
    total_tests = len(test_cases)
    
    for input_song, expected_similar in test_cases.items():
        results = search_similar_songs(input_song, top_k=3)
        
        # Check if at least 1 expected song in top-3 results
        found_similar = any(song in [r['filename'] for r in results] 
                          for song in expected_similar)
        
        if found_similar:
            correct_predictions += 1
    
    accuracy = correct_predictions / total_tests
    return accuracy
```

#### Cross-validation Strategy
1. **Leave-one-out**: Remove 1 song, test against remaining 129
2. **Genre-based split**: Train on 8 genres, test on 2
3. **Temporal split**: Use different recording sessions

### üìà Performance Metrics

#### Precision & Recall
```python
def calculate_precision_recall(predicted, ground_truth):
    true_positives = len(set(predicted) & set(ground_truth))
    
    precision = true_positives / len(predicted) if predicted else 0
    recall = true_positives / len(ground_truth) if ground_truth else 0
    
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    return precision, recall, f1_score
```

#### Mean Reciprocal Rank (MRR)
```python
def calculate_mrr(results_list):
    """Measure how high relevant results appear in ranking"""
    reciprocal_ranks = []
    
    for results in results_list:
        for rank, item in enumerate(results, 1):
            if item['is_relevant']:
                reciprocal_ranks.append(1.0 / rank)
                break
        else:
            reciprocal_ranks.append(0.0)
    
    return np.mean(reciprocal_ranks)
```

---

## üîÆ FUTURE IMPROVEMENTS

### üéµ Advanced Audio Features

#### 1. MFCC (Mel-Frequency Cepstral Coefficients)
```python
def extract_mfcc(audio, sr, n_mfcc=13):
    """Extract perceptually-motivated features"""
    # Mel-scale frequency banks
    mel_filters = create_mel_filterbank(sr)
    
    # Apply filters to spectrum
    mel_spectrum = np.dot(mel_filters, np.abs(np.fft.rfft(audio))**2)
    
    # Log and DCT transform
    log_mel = np.log(mel_spectrum + 1e-8)
    mfcc = dct(log_mel)[:n_mfcc]
    
    return mfcc
```

#### 2. Chroma Features
```python
def extract_chroma(audio, sr):
    """Extract pitch class distribution"""
    # Map frequencies to 12 pitch classes (C, C#, D, ...)
    chroma = np.zeros(12)
    
    freqs = np.fft.rfftfreq(len(audio), 1/sr)
    spectrum = np.abs(np.fft.rfft(audio))
    
    for i, freq in enumerate(freqs):
        if freq > 0:
            # Convert frequency to pitch class
            pitch_class = int(np.round(12 * np.log2(freq / 440.0))) % 12
            chroma[pitch_class] += spectrum[i]
    
    return chroma / np.sum(chroma)  # Normalize
```

#### 3. Tonnetz (Harmonic Network)
```python
def extract_tonnetz(chroma):
    """Extract tonal centroid features"""
    # Define harmonic change vectors
    r1 = np.array([1, 0, -1, 0, 1, 0, -1, 0, 1, 0, -1, 0])  # Perfect fifths
    r2 = np.array([0, 1, 0, -1, 0, 1, 0, -1, 0, 1, 0, -1])  # Minor thirds
    r3 = np.array([1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1]) # Major thirds
    
    tonnetz = np.array([
        np.dot(chroma, r1),
        np.dot(chroma, r2), 
        np.dot(chroma, r3)
    ])
    
    return tonnetz
```

### ü§ñ Machine Learning Enhancements

#### 1. Deep Audio Embeddings
```python
# Using pre-trained models like VGGish, OpenL3, or Wav2Vec2
import tensorflow as tf

def extract_deep_features(audio_path):
    """Extract high-level semantic features"""
    model = tf.keras.models.load_model('vggish_model.h5')
    
    # Preprocess audio to model format
    audio_preprocessed = preprocess_for_vggish(audio_path)
    
    # Extract 128-dimensional embeddings
    embeddings = model.predict(audio_preprocessed)
    
    return embeddings.flatten()
```

#### 2. Metric Learning
```python
# Learn optimal distance metric for music similarity
from sklearn.neighbors import NeighborhoodComponentsAnalysis

def learn_similarity_metric(features, labels):
    """Learn transformation that improves similarity"""
    nca = NeighborhoodComponentsAnalysis(n_components=50)
    
    # Transform features to optimize k-NN classification
    features_transformed = nca.fit_transform(features, labels)
    
    return nca, features_transformed
```

### üèóÔ∏è Scalability Improvements

#### 1. Approximate Nearest Neighbors
```python
import faiss

def build_faiss_index(features):
    """Build fast similarity search index"""
    dimension = features.shape[1]
    
    # Create FAISS index
    index = faiss.IndexFlatIP(dimension)  # Inner product (cosine)
    
    # Add vectors to index
    index.add(features.astype('float32'))
    
    return index

def search_with_faiss(index, query_vector, k=10):
    """Fast approximate search"""
    similarities, indices = index.search(
        query_vector.reshape(1, -1).astype('float32'), 
        k
    )
    return similarities[0], indices[0]
```

#### 2. Hierarchical Clustering
```python
from sklearn.cluster import AgglomerativeClustering

def create_music_hierarchy(features, n_clusters=20):
    """Create hierarchical organization of songs"""
    clustering = AgglomerativeClustering(
        n_clusters=n_clusters,
        linkage='ward'
    )
    
    cluster_labels = clustering.fit_predict(features)
    
    # Create cluster-based search
    clusters = {}
    for i, label in enumerate(cluster_labels):
        if label not in clusters:
            clusters[label] = []
        clusters[label].append(i)
    
    return clusters
```

---

## üìö C√ÇU H·ªéI V·∫§N ƒê√ÅP CHUY√äN S√ÇU

### üî¨ Technical Deep Dive Questions

**Q: Gi·∫£i th√≠ch chi ti·∫øt process tr√≠ch xu·∫•t spectral centroid?**

A: Spectral centroid ƒë∆∞·ª£c t√≠nh qua c√°c b∆∞·ªõc:

1. **Windowing**: √Åp d·ª•ng Hamming window ƒë·ªÉ gi·∫£m spectral leakage
```python
window = np.hamming(len(audio_segment))
windowed_signal = audio_segment * window
```

2. **FFT Transform**: Chuy·ªÉn t·ª´ time domain sang frequency domain
```python
spectrum = np.abs(np.fft.rfft(windowed_signal))
frequencies = np.fft.rfftfreq(len(windowed_signal), 1/sample_rate)
```

3. **Weighted Average**: T√≠nh tr·ªçng t√¢m t·∫ßn s·ªë
```python
total_energy = np.sum(spectrum)
if total_energy > 0:
    centroid = np.sum(frequencies * spectrum) / total_energy
```

4. **Physical Meaning**: Centroid cao ‚Üí √¢m thanh "s√°ng", nhi·ªÅu high-frequency components

**Q: T·∫°i sao overlap 50% cho bars? C√≥ th·ªÉ optimize kh√¥ng?**

A: 50% overlap balance gi·ªØa detail v√† efficiency:

**Advantages c·ªßa 50%:**
- Ensure kh√¥ng miss information ·ªü bar boundaries  
- Smooth temporal transitions
- Standard trong audio processing literature

**Alternative overlaps:**
- **25% overlap**: Faster, √≠t redundancy, c√≥ th·ªÉ miss transients
- **75% overlap**: More detail, nh∆∞ng 3x slower processing
- **Variable overlap**: Adaptive d·ª±a tr√™n onset detection

**Optimization strategy:**
```python
def adaptive_overlap(audio, onset_times):
    """Vary overlap based on musical structure"""
    overlaps = []
    for i in range(len(onset_times)-1):
        segment_length = onset_times[i+1] - onset_times[i]
        
        if segment_length < 1.0:  # Short segments
            overlap = 0.75  # More overlap
        elif segment_length > 3.0:  # Long segments  
            overlap = 0.25  # Less overlap
        else:
            overlap = 0.50  # Standard
            
        overlaps.append(overlap)
    
    return overlaps
```

**Q: Database c√≥ handle concurrent access kh√¥ng? Scalability?**

A: Current SQLite setup basic, c√≥ limitations:

**Current Limitations:**
- Single-writer, multiple-readers
- File-based locking
- No distributed capability

**Production Improvements:**
```python
# 1. Connection pooling
class DatabasePool:
    def __init__(self, db_path, pool_size=10):
        self.pool = queue.Queue()
        for _ in range(pool_size):
            conn = sqlite3.connect(db_path, check_same_thread=False)
            self.pool.put(conn)
    
    def get_connection(self):
        return self.pool.get()
    
    def return_connection(self, conn):
        self.pool.put(conn)

# 2. Read replicas
class ReadWriteDatabase:
    def __init__(self, write_db, read_dbs):
        self.write_db = write_db
        self.read_dbs = read_dbs  # List of read replicas
        self.read_index = 0
    
    def read_query(self, query):
        # Load balance across read replicas
        db = self.read_dbs[self.read_index % len(self.read_dbs)]
        self.read_index += 1
        return db.execute(query)

# 3. Distributed approach (PostgreSQL + Redis)
import redis
import psycopg2

class DistributedMusicDB:
    def __init__(self):
        self.postgres = psycopg2.connect(...)  # Metadata
        self.redis = redis.Redis(...)          # Feature vectors cache
    
    def store_song(self, song_data, features):
        # Store metadata in PostgreSQL
        self.postgres.execute(
            "INSERT INTO songs (filename, genre) VALUES (%s, %s)",
            (song_data['filename'], song_data['genre'])
        )
        
        # Cache features in Redis
        self.redis.set(
            f"features:{song_data['id']}", 
            pickle.dumps(features)
        )
```

**Q: L√†m sao evaluate quality c·ªßa similarity results?**

A: Multiple evaluation strategies:

**1. Objective Metrics:**
```python
def evaluate_retrieval_quality(test_queries):
    metrics = {
        'precision_at_k': [],
        'recall_at_k': [],
        'map_score': [],  # Mean Average Precision
        'mrr_score': []   # Mean Reciprocal Rank
    }
    
    for query in test_queries:
        results = search_similar_songs(query['audio'])
        ground_truth = query['similar_songs']
        
        # Calculate metrics
        p_at_k = precision_at_k(results, ground_truth, k=5)
        r_at_k = recall_at_k(results, ground_truth, k=5)
        
        metrics['precision_at_k'].append(p_at_k)
        metrics['recall_at_k'].append(r_at_k)
    
    return {k: np.mean(v) for k, v in metrics.items()}
```

**2. Human Evaluation:**
```python
def human_evaluation_interface():
    """Web interface for human similarity rating"""
    # Present query song + top-5 results
    # Ask humans to rate similarity 1-5 scale
    # Calculate inter-annotator agreement
    # Use as ground truth for model evaluation
```

**3. Cross-validation:**
```python
def genre_based_validation():
    """Test generalization across genres"""
    genres = ['blues', 'classical', 'rock', 'jazz']
    
    for test_genre in genres:
        train_songs = [s for s in all_songs if s.genre != test_genre]
        test_songs = [s for s in all_songs if s.genre == test_genre]
        
        # Build index from train songs
        train_features = extract_features(train_songs)
        
        # Test on test songs
        for test_song in test_songs:
            results = search_in_features(test_song, train_features)
            # Evaluate if results make sense
```

**Q: Feature vector 151-D c√≥ curse of dimensionality kh√¥ng?**

A: C√≥ considerations, nh∆∞ng manageable:

**Curse of Dimensionality Issues:**
- Distance metrics become less discriminative
- Nearest neighbors tend to be equally far
- Need exponentially more data

**Why 151-D works cho music:**
1. **Structured dimensions**: Kh√¥ng random, c√≥ musical meaning
2. **Redundancy**: C√°c bars c√≥ correlation, gi·∫£m effective dimensionality  
3. **Domain-specific**: Audio features c√≥ natural clustering

**Mitigation strategies:**
```python
# 1. Dimensionality reduction
from sklearn.decomposition import PCA

def reduce_dimensions(features, n_components=50):
    """Reduce to most important dimensions"""
    pca = PCA(n_components=n_components)
    features_reduced = pca.fit_transform(features)
    
    # Check explained variance
    explained_var = np.sum(pca.explained_variance_ratio_)
    print(f"Retained {explained_var:.2%} of variance")
    
    return features_reduced, pca

# 2. Feature selection
from sklearn.feature_selection import SelectKBest, mutual_info_regression

def select_best_features(features, labels, k=50):
    """Select most informative features"""
    selector = SelectKBest(mutual_info_regression, k=k)
    features_selected = selector.fit_transform(features, labels)
    
    selected_indices = selector.get_support(indices=True)
    return features_selected, selected_indices

# 3. Locality Sensitive Hashing
import numpy as np

class LSH:
    """Approximate nearest neighbors for high-D data"""
    def __init__(self, n_hashes=10, n_bits=8):
        self.n_hashes = n_hashes
        self.n_bits = n_bits
        self.hash_functions = []
        
    def fit(self, data):
        """Create random projection hash functions"""
        d = data.shape[1]
        
        for _ in range(self.n_hashes):
            # Random projection matrix
            projection = np.random.randn(d, self.n_bits)
            self.hash_functions.append(projection)
    
    def hash_vector(self, vector):
        """Convert vector to hash codes"""
        hashes = []
        for projection in self.hash_functions:
            # Project and threshold
            projected = np.dot(vector, projection)
            hash_code = (projected > 0).astype(int)
            hashes.append(hash_code)
        return hashes
```

---

## üéØ DEMO PRESENTATION STRATEGY

### üé™ Live Demo Flow

1. **Opening Hook** (30 seconds)
   - "C√≥ bao gi·ªù b·∫°n nghe m·ªôt b√†i nh·∫°c v√† mu·ªën t√¨m nh·ªØng b√†i t∆∞∆°ng t·ª±?"
   - Show demo interface tr∆∞·ªõc

2. **Technical Overview** (2 minutes)
   - Quick architecture diagram
   - Highlight key innovations: bar-based, 151 features

3. **Live Demo** (3 minutes)
   - Upload blues song ‚Üí show blues results
   - Upload classical ‚Üí show classical results  
   - Upload rock ‚Üí show mixed results (interesting case)

4. **Deep Dive** (5 minutes)
   - Code walkthrough c·ªßa key functions
   - Show feature extraction process
   - Explain similarity calculation

5. **Q&A Preparation** (remaining time)
   - Ready for technical questions
   - Have backup examples prepared

### üéØ Key Messages to Emphasize

1. **Innovation**: Bar-based approach v·ªõi overlap
2. **Robustness**: Multiple complementary features
3. **Performance**: Real-time processing
4. **Scalability**: Clear path to production scale
5. **Accuracy**: Good results cho demo dataset

### üîß Backup Plans

**If demo fails:**
- Have pre-recorded screen capture
- Static screenshots c·ªßa results
- Code examples c√≥ th·ªÉ run offline

**If technical questions too deep:**
- Acknowledge limitations honestly
- Show improvement roadmap
- Redirect to working components

---

**üéµ Good luck v·ªõi presentation! Remember: Be confident, explain clearly, v√† show passion for the technical challenges! üéµ** 